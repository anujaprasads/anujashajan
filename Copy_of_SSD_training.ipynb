{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anujaprasads/anujashajan/blob/main/Copy_of_SSD_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gfrHyQLwY9v"
      },
      "source": [
        "# Single Shot Detector Training\n",
        "Training a *Single Shot Detector* on custom dataset to detect road traffic\n",
        "\n",
        ">Reference: [Note on Everything](https://noteoneverything.blogspot.com/2018/03/how-to-use-ssd-single-shot-multibox.html)\n",
        "\n",
        "---\n",
        "## Download Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiv7yfSp8QHQ"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "#gauth.LocalWebserverAuth()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "def save_file_to_drive(name, path):\n",
        "    file_metadata = {\n",
        "      'name': name,\n",
        "      'mimeType': 'application/octet-stream'\n",
        "     }\n",
        "\n",
        "    media = MediaFileUpload(path,\n",
        "                    mimetype='application/octet-stream',\n",
        "                    resumable=True)\n",
        "\n",
        "    created = drive_service.files().create(body=file_metadata,\n",
        "                                   media_body=media,\n",
        "                                   fields='id').execute()\n",
        "\n",
        "    print('File ID: {}'.format(created.get('id')))\n",
        "\n",
        "    return created\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# choose a local (colab) directory to store the data.\n",
        "local_download_path = os.path.expanduser('./')\n",
        "try:\n",
        "    os.makedirs(local_download_path)\n",
        "except: pass\n",
        "\n",
        "# 2. Auto-iterate using the query syntax\n",
        " #  https://developers.google.com/drive/v2/web/search-parameters\n",
        "print('jj')\n",
        "\"\"\"file_list = drive.ListFile(\n",
        "    {'q': \"'17iTm3hgEyv_t_MThqhbyWU7_k08_u8Ne'' in parents\"}).GetList()\n",
        "\n",
        "for f in file_list:\n",
        "  # 3. Create & download by id.\n",
        "    print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "    fname = os.path.join(local_download_path, f['title'])\n",
        "    print('downloading to {}'.format(fname))\n",
        "    f_ = drive.CreateFile({'id': f['id']})\n",
        "    f_.GetContentFile(fname)\n",
        "\n",
        "\n",
        "with open(fname, 'r') as f:\n",
        "    print(f.read())\"\"\"\n",
        "\n",
        "\n",
        "def ListFolder(parent):\n",
        "  filelist=[]\n",
        "  file_list = drive.ListFile({'q': \"'%s' in parents and trashed=false\" % parent}).GetList()\n",
        "  for f in file_list:\n",
        "    if f['mimeType']=='application/vnd.google-apps.folder': # if folder\n",
        "        filelist.append({\"id\":f['id'],\"title\":f['title'],\"list\":ListFolder(f['id'])})\n",
        "    else:\n",
        "        filelist.append({\"title\":f['title'],\"title1\":f['alternateLink']})\n",
        "  return filelist\n",
        "\n",
        "ListFolder('root')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q_Dt_36l1Y9"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "def save_file_to_drive(name, path):\n",
        "    file_metadata = {\n",
        "      'name': name,\n",
        "      'mimeType': 'application/octet-stream'\n",
        "     }\n",
        "\n",
        "    media = MediaFileUpload(path,\n",
        "                    mimetype='application/octet-stream',\n",
        "                    resumable=True)\n",
        "\n",
        "    created = drive_service.files().create(body=file_metadata,\n",
        "                                   media_body=media,\n",
        "                                   fields='id').execute()\n",
        "\n",
        "    print('File ID: {}'.format(created.get('id')))\n",
        "\n",
        "    return created\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# choose a local (colab) directory to store the data.\n",
        "local_download_path = os.path.expanduser('./')\n",
        "try:\n",
        "    os.makedirs(local_download_path)\n",
        "except: pass\n",
        "\n",
        "# 2. Auto-iterate using the query syntax\n",
        "#    https://developers.google.com/drive/v2/web/search-parameters\n",
        "file_list = drive.ListFile(\n",
        "    {'q': \"'1QisE8jL6IgiKXgBesZmhKz87Htn9LHii' in parents\"}).GetList()\n",
        "\n",
        "for f in file_list:\n",
        "  # 3. Create & download by id.\n",
        "    print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "    fname = os.path.join(local_download_path, f['title'])\n",
        "    print('downloading to {}'.format(fname))\n",
        "    f_ = drive.CreateFile({'id': f['id']})\n",
        "    f_.GetContentFile(fname)\n",
        "\n",
        "\n",
        "with open(fname, 'r') as f:\n",
        "    print(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZMKqpgQl-m3"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# choose a local (colab) directory to store the data.\n",
        "local_download_path = os.path.expanduser('~/data')\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass\n",
        "\n",
        "# 2. Auto-iterate using the query syntax\n",
        "#    https://developers.google.com/drive/v2/web/search-parameters\n",
        "file_list = drive.ListFile(\n",
        "    {'q': \"'1aGW8Ww1jDtw6T_8YTJVnZLN11L8GvxLg' in parents\"}).GetList()\n",
        "\n",
        "for f in file_list:\n",
        "  # 3. Create & download by id.\n",
        "  print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "  fname = os.path.join(local_download_path, f['title'])\n",
        "  print('downloading to {}'.format(fname))\n",
        "  f_ = drive.CreateFile({'id': f['id']})\n",
        "  f_.GetContentFile(fname)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02CnB0xVaHcU"
      },
      "source": [
        "#!pip install tensorflow==2.4\n",
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from googleapiclient.discovery import build\n",
        "import numpy as np\n",
        "import cv2\n",
        "import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "import sklearn.preprocessing\n",
        "import pickle\n",
        "import pathlib\n",
        "import glob, os\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "def save_file_to_drive(name, path):\n",
        "    file_metadata = {\n",
        "      'name': name,\n",
        "      'mimeType': 'application/octet-stream'\n",
        "     }\n",
        "\n",
        "    media = MediaFileUpload(path,\n",
        "                    mimetype='application/octet-stream',\n",
        "                    resumable=True)\n",
        "\n",
        "    created = drive_service.files().create(body=file_metadata,\n",
        "                                   media_body=media,\n",
        "                                   fields='id').execute()\n",
        "\n",
        "    print('File ID: {}'.format(created.get('id')))\n",
        "\n",
        "    return created\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# choose a local (colab) directory to store the data.\n",
        "local_download_path = os.path.expanduser('./')\n",
        "try:\n",
        "    os.makedirs(local_download_path)\n",
        "except: pass\n",
        "\n",
        "# 2. Auto-iterate using the query syntax\n",
        "#    https://developers.google.com/drive/v2/web/search-parameters\n",
        "file_list = drive.ListFile(\n",
        "    {'q': \"'1TGt7Ht2csaFnUd6AXn49rfX-x1Sh9dFI' in parents\"}).GetList()\n",
        "\n",
        "for f in file_list:\n",
        "    print(f['title'])\n",
        "\n",
        "  # 3. Create & download by id.\n",
        "\n",
        "    fname = os.path.join(local_download_path, f['title'])\n",
        "    print('downloading to {}'.format(fname))\n",
        "    cwd = os.getcwd()\n",
        "    print(cwd)\n",
        "    print('deek')\n",
        "\n",
        "    f_ = drive.CreateFile({'id': f['id']})\n",
        "    f_.GetContentFile(fname)\n",
        "\n",
        "\n",
        "with open(fname, 'r') as f:\n",
        "    print(f.read())\n",
        "\n",
        "\n",
        "#import imread\n",
        "# Load an color image in grayscale\n",
        "#!mkdir abc\n",
        "#!echo \"file\" > abc/123.txt\n",
        "\n",
        "#import os\n",
        "#os.chdir('abc')\n",
        "#! ls\n",
        "\n",
        "#print( os.listdir('data') )\n",
        "#files.download( \"data/dm.ckpt.meta\" )\n",
        "#filecount=0;\n",
        "#folder=pathlib.Path('D:/DhanalekshmiVehicleClassification/MAINDATASET6/draining/wow/')\n",
        "#print(folder)\n",
        "#cwd = os.getcwd()\n",
        "#print(cwd)\n",
        "#print(\"deek\")\n",
        "#for x in range(0, 3):\n",
        " #   print(folder)\n",
        "\n",
        "#for file in folder.glob(\"*.txt\")\n",
        "\n",
        "#for file in folder.glob(\"*.jpg\"):\n",
        " #   print('dek1')\n",
        "  #  im = cv2.imread('*.jpg',0)\n",
        "   # file = open(\"data.pkl\", \"wb\")\n",
        "    #pickle.dump(im, file)\n",
        "    #filecount+=1\n",
        "    #file.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hdRYY66e7oP"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYZ02p_Ce8PB"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQjf56P8e9Ua"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzFhWtN_Qkpk"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAj8NAUSGrOw"
      },
      "source": [
        "!dir\n",
        "!tar -xf labelImgmaster.tar\n",
        "!dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g33DKjOWKLs3"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCb-UIKkwY9_"
      },
      "source": [
        "## Copy files to google drive\n",
        "*This section is required to copy checkpoints to google drive when running it on [Google Colaboratory](https://colab.research.google.com)*\n",
        "\n",
        "### 1. Authenticate user"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGkX-izjrya-"
      },
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from googleapiclient.discovery import build\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFAz_L16wY-K"
      },
      "source": [
        "### 2. Define the save function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t39BlPQpr9JE"
      },
      "source": [
        "drive_service = build('drive', 'v3')\n",
        "def save_file_to_drive(name, path):\n",
        "    file_metadata = {\n",
        "      'name': name,\n",
        "      'mimeType': 'application/octet-stream'\n",
        "     }\n",
        "\n",
        "    media = MediaFileUpload(path,\n",
        "                    mimetype='application/octet-stream',\n",
        "                    resumable=True)\n",
        "\n",
        "    created = drive_service.files().create(body=file_metadata,\n",
        "                                   media_body=media,\n",
        "                                   fields='id').execute()\n",
        "\n",
        "    print('File ID: {}'.format(created.get('id')))\n",
        "\n",
        "    return created"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOFTvJJNwY-S"
      },
      "source": [
        "## Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-2dwRlP4KmY"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59OBaRWNly1d"
      },
      "source": [
        "#!cd /root/data/ssd\n",
        "import cv2\n",
        "import keras\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from tensorflow.compat.v1.keras.backend import set_session\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "\n",
        "config = ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "!pip install scipy-0.17.1-cp27-cp27m-win32.whl\n",
        "!pip install scipy==1.1.0\n",
        "from random import shuffle\n",
        "from scipy.misc.pilutil import imread\n",
        "from scipy.misc.pilutil  import imresize\n",
        "import tensorflow as tf\n",
        "\n",
        "from ssdv2 import SSD300v2 as SSD300\n",
        "from ssd_training import MultiboxLoss\n",
        "from ssd_utils import BBoxUtility\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (8, 8)\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "\n",
        "np.set_printoptions(suppress=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mguVFxM0wY-Z"
      },
      "source": [
        "## Define some constants and load pkl file   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1nmE_gbly5q"
      },
      "source": [
        "# some constants\n",
        "NUM_CLASSES = 5\n",
        "input_shape = (300, 300, 3)\n",
        "print(NUM_CLASSES )\n",
        "!pip remove tensorflow-gpu tensorflow tensorflow-base\n",
        "!pip install tensorflow\n",
        "priors = pickle.load(open('prior_boxes_ssd300.pkl', 'rb'))\n",
        "bbox_util = BBoxUtility(NUM_CLASSES, priors)\n",
        "#python3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiRENmmrly7s"
      },
      "source": [
        "!pip3 install --upgrade --force-reinstall tensorflow-gpu==1.15.0\n",
        "priors = pickle.load(open('prior_boxes_ssd300.pkl', 'rb'))\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "config = ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
        "config.gpu_options.allow_growth = True\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "session = InteractiveSession(config=config)\n",
        "bbox_util = BBoxUtility(NUM_CLASSES, priors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwYHHlWJKTp3"
      },
      "source": [
        "!dir\n",
        "!tar -xf ssd5testingblob.tar\n",
        "!dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWkVS0kxly-Q"
      },
      "source": [
        "import pickle\n",
        "gt = pickle.load(open('traffic_sign.pkl', 'rb'))\n",
        "keys = sorted(gt.keys())\n",
        "num_train = int(round(0.8 * len(keys)))\n",
        "train_keys = keys[:num_train]\n",
        "val_keys = keys[num_train:]\n",
        "num_val = len(val_keys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQIAdBokKSAn"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18_tbMEcwY-x"
      },
      "source": [
        "## Define Generator Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcqQ0hYOlzA6"
      },
      "source": [
        "class Generator(object):\n",
        "    def __init__(self, gt, bbox_util,\n",
        "                 batch_size, path_prefix,\n",
        "                 train_keys, val_keys, image_size,\n",
        "                 saturation_var=0.5,\n",
        "                 brightness_var=0.5,\n",
        "                 contrast_var=0.5,\n",
        "                 lighting_std=0.5,\n",
        "                 hflip_prob=0.5,\n",
        "                 vflip_prob=0.5,\n",
        "                 do_crop=True,\n",
        "                 crop_area_range=[0.75, 1.0],\n",
        "                 aspect_ratio_range=[3./4., 4./3.]):\n",
        "        self.gt = gt\n",
        "        self.bbox_util = bbox_util\n",
        "        self.batch_size = batch_size\n",
        "        self.path_prefix = path_prefix\n",
        "        self.train_keys = train_keys\n",
        "        self.val_keys = val_keys\n",
        "        self.train_batches = len(train_keys)\n",
        "        self.val_batches = len(val_keys)\n",
        "        self.image_size = image_size\n",
        "        self.color_jitter = []\n",
        "        if saturation_var:\n",
        "            self.saturation_var = saturation_var\n",
        "            self.color_jitter.append(self.saturation)\n",
        "        if brightness_var:\n",
        "            self.brightness_var = brightness_var\n",
        "            self.color_jitter.append(self.brightness)\n",
        "        if contrast_var:\n",
        "            self.contrast_var = contrast_var\n",
        "            self.color_jitter.append(self.contrast)\n",
        "        self.lighting_std = lighting_std\n",
        "        self.hflip_prob = hflip_prob\n",
        "        self.vflip_prob = vflip_prob\n",
        "        self.do_crop = do_crop\n",
        "        self.crop_area_range = crop_area_range\n",
        "        self.aspect_ratio_range = aspect_ratio_range\n",
        "\n",
        "    def grayscale(self, rgb):\n",
        "        return rgb.dot([0.299, 0.587, 0.114])\n",
        "\n",
        "    def saturation(self, rgb):\n",
        "        gs = self.grayscale(rgb)\n",
        "        alpha = 2 * np.random.random() * self.saturation_var\n",
        "        alpha += 1 - self.saturation_var\n",
        "        rgb = rgb * alpha + (1 - alpha) * gs[:, :, None]\n",
        "        return np.clip(rgb, 0, 255)\n",
        "\n",
        "    def brightness(self, rgb):\n",
        "        alpha = 2 * np.random.random() * self.brightness_var\n",
        "        alpha += 1 - self.saturation_var\n",
        "        rgb = rgb * alpha\n",
        "        return np.clip(rgb, 0, 255)\n",
        "\n",
        "    def contrast(self, rgb):\n",
        "        gs = self.grayscale(rgb).mean() * np.ones_like(rgb)\n",
        "        alpha = 2 * np.random.random() * self.contrast_var\n",
        "        alpha += 1 - self.contrast_var\n",
        "        rgb = rgb * alpha + (1 - alpha) * gs\n",
        "        return np.clip(rgb, 0, 255)\n",
        "\n",
        "    def lighting(self, img):\n",
        "        cov = np.cov(img.reshape(-1, 3) / 255.0, rowvar=False)\n",
        "        eigval, eigvec = np.linalg.eigh(cov)\n",
        "        noise = np.random.randn(3) * self.lighting_std\n",
        "        noise = eigvec.dot(eigval * noise) * 255\n",
        "        img += noise\n",
        "        return np.clip(img, 0, 255)\n",
        "\n",
        "    def horizontal_flip(self, img, y):\n",
        "        if np.random.random() < self.hflip_prob:\n",
        "            img = img[:, ::-1]\n",
        "            y[:, [0, 2]] = 1 - y[:, [2, 0]]\n",
        "        return img, y\n",
        "\n",
        "    def vertical_flip(self, img, y):\n",
        "        if np.random.random() < self.vflip_prob:\n",
        "            img = img[::-1]\n",
        "            y[:, [1, 3]] = 1 - y[:, [3, 1]]\n",
        "        return img, y\n",
        "\n",
        "    def random_sized_crop(self, img, targets):\n",
        "        img_w = img.shape[1]\n",
        "        img_h = img.shape[0]\n",
        "        img_area = img_w * img_h\n",
        "        random_scale = np.random.random()\n",
        "        random_scale *= (self.crop_area_range[1] -\n",
        "                         self.crop_area_range[0])\n",
        "        random_scale += self.crop_area_range[0]\n",
        "        target_area = random_scale * img_area\n",
        "        random_ratio = np.random.random()\n",
        "        random_ratio *= (self.aspect_ratio_range[1] -\n",
        "                         self.aspect_ratio_range[0])\n",
        "        random_ratio += self.aspect_ratio_range[0]\n",
        "        w = np.round(np.sqrt(target_area * random_ratio))\n",
        "        h = np.round(np.sqrt(target_area / random_ratio))\n",
        "        if np.random.random() < 0.5:\n",
        "            w, h = h, w\n",
        "        w = min(w, img_w)\n",
        "        w_rel = w / img_w\n",
        "        w = int(w)\n",
        "        h = min(h, img_h)\n",
        "        h_rel = h / img_h\n",
        "        h = int(h)\n",
        "        x = np.random.random() * (img_w - w)\n",
        "        x_rel = x / img_w\n",
        "        x = int(x)\n",
        "        y = np.random.random() * (img_h - h)\n",
        "        y_rel = y / img_h\n",
        "        y = int(y)\n",
        "        img = img[y:y+h, x:x+w]\n",
        "        new_targets = []\n",
        "        for box in targets:\n",
        "            cx = 0.5 * (box[0] + box[2])\n",
        "            cy = 0.5 * (box[1] + box[3])\n",
        "            if (x_rel < cx < x_rel + w_rel and\n",
        "                y_rel < cy < y_rel + h_rel):\n",
        "                xmin = (box[0] - x_rel) / w_rel\n",
        "                ymin = (box[1] - y_rel) / h_rel\n",
        "                xmax = (box[2] - x_rel) / w_rel\n",
        "                ymax = (box[3] - y_rel) / h_rel\n",
        "                xmin = max(0, xmin)\n",
        "                ymin = max(0, ymin)\n",
        "                xmax = min(1, xmax)\n",
        "                ymax = min(1, ymax)\n",
        "                box[:4] = [xmin, ymin, xmax, ymax]\n",
        "                new_targets.append(box)\n",
        "        new_targets = np.asarray(new_targets).reshape(-1, targets.shape[1])\n",
        "        return img, new_targets\n",
        "\n",
        "    def generate(self, train=True):\n",
        "        while True:\n",
        "            if train:\n",
        "                shuffle(self.train_keys)\n",
        "                keys = self.train_keys\n",
        "            else:\n",
        "                shuffle(self.val_keys)\n",
        "                keys = self.val_keys\n",
        "            inputs = []\n",
        "            targets = []\n",
        "            for key in keys:\n",
        "                img_path = self.path_prefix + key\n",
        "                img = imread(img_path).astype('float32')\n",
        "                y = self.gt[key].copy()\n",
        "                if train and self.do_crop:\n",
        "                    img, y = self.random_sized_crop(img, y)\n",
        "                img = imresize(img, self.image_size).astype('float32')\n",
        "                if train:\n",
        "                    shuffle(self.color_jitter)\n",
        "                    for jitter in self.color_jitter:\n",
        "                        img = jitter(img)\n",
        "                    if self.lighting_std:\n",
        "                        img = self.lighting(img)\n",
        "                    if self.hflip_prob > 0:\n",
        "                        img, y = self.horizontal_flip(img, y)\n",
        "                    if self.vflip_prob > 0:\n",
        "                        img, y = self.vertical_flip(img, y)\n",
        "                y = self.bbox_util.assign_boxes(y)\n",
        "                inputs.append(img)\n",
        "                targets.append(y)\n",
        "                if len(targets) == self.batch_size:\n",
        "                    tmp_inp = np.array(inputs)\n",
        "                    tmp_targets = np.array(targets)\n",
        "                    inputs = []\n",
        "                    targets = []\n",
        "                    yield preprocess_input(tmp_inp), tmp_targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7Tm0A_5wY-8"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1nGGAZulzCZ"
      },
      "source": [
        "path_prefix =  'ssd5testingblob/new_dataset/traffic_sign/'\n",
        "gen = Generator(gt, bbox_util, 1, 'ssd5testingblob/new_dataset/traffic_sign/',\n",
        "                train_keys, val_keys,\n",
        "                (input_shape[0], input_shape[1]), do_crop=False)\n",
        "print(path_prefix)\n",
        "print(gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiiR9zRzlzEL"
      },
      "source": [
        "# Define the model\n",
        "model = SSD300(input_shape, num_classes=NUM_CLASSES)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG4RyenilzGF"
      },
      "source": [
        "freeze = ['input_1', 'conv1_1', 'conv1_2', 'pool1',\n",
        "          'conv2_1', 'conv2_2', 'pool2',\n",
        "          'conv3_1', 'conv3_2', 'conv3_3', 'pool3']\n",
        "\n",
        "for L in model.layers:\n",
        "    if L.name in freeze:\n",
        "        L.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAPTFlOTlzIU"
      },
      "source": [
        "def schedule(epochs, decay=0.9):\n",
        "    return base_lr * decay**(epochs)\n",
        "\n",
        "callbacks = [keras.callbacks.ModelCheckpoint('./checkpoints/weights.{epochs:02d}-{val_loss:.2f}.hdf5',\n",
        "                                             monitor='val_loss',\n",
        "                                             verbose=1,\n",
        "                                             save_best_only=True,\n",
        "                                             mode='min'),\n",
        "             keras.callbacks.LearningRateScheduler(schedule)]\n",
        "print(callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTY0uGf4lzKW"
      },
      "source": [
        "base_lr = 3e-4\n",
        "optim = keras.optimizers.Adam(lr=base_lr)\n",
        "\n",
        "model.compile(optimizer=optim,\n",
        "              loss=MultiboxLoss(NUM_CLASSES, neg_pos_ratio=2.0).compute_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GYZxXpFlzNN"
      },
      "source": [
        "epochs = 20\n",
        "\n",
        "\n",
        "#history = model.fit_generator(gen.generate(True), gen.train_batches,\n",
        "                            #epochs, verbose=1,\n",
        "                              #callbacks=callbacks,\n",
        "                             #validation_steps=gen.generate(False),\n",
        "                              #steps=gen.val_batches, nb_worker=1)\n",
        "                             #steps=gen.val_batches,\n",
        "history =model.fit_generator(\n",
        "                gen.generate(True), gen.train_batches,\n",
        "                #steps_per_epoch=train_samples // batch_size,\n",
        "                epochs=epochs,\n",
        "                validation_data=gen.generate(False),\n",
        "                validation_steps=gen.val_batches\n",
        "               ,nb_worker=1)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwsIbY-3wY_e"
      },
      "source": [
        "---\n",
        "### 3. Create a .tar archive of the checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQXcWPjasV49"
      },
      "source": [
        "#!tar -cvf ssdckpt.tar checkpoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mza-LhOmwY_q"
      },
      "source": [
        "### 4. Save checkpoints to google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4nje5LYsXi6"
      },
      "source": [
        "#save_file_to_drive('ssd-checkpoints.tar', 'ssdckpt.tar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAsPgFKxwZAB"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB8QmtKtlzPD"
      },
      "source": [
        "inputs = []\n",
        "images = []\n",
        "# Define the test image\n",
        "img_path = \"/content/sample_data/frame0.jpg\"\n",
        "print(sorted(val_keys)[0])\n",
        "img = image.load_img(img_path,target_size=(300, 300))\n",
        "img = image.img_to_array(img)\n",
        "images.append(imread(img_path))\n",
        "inputs.append(img.copy())\n",
        "inputs = preprocess_input(np.array(inputs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXm5k_RvlzQh"
      },
      "source": [
        "preds = model.predict(inputs, batch_size=1, verbose=1)\n",
        "results = bbox_util.detection_out(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkEnEXtXJO_L"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyiXz20EJO9j"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSFEHzhNJJBS"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUbFRErclzRq"
      },
      "source": [
        "for i, img in enumerate(images):\n",
        "    # Parse the outputs.\n",
        "    det_label = results[i][:, 0]\n",
        "    det_conf = results[i][:, 1]\n",
        "    det_xmin = results[i][:, 2]\n",
        "    det_ymin = results[i][:, 3]\n",
        "    det_xmax = results[i][:, 4]\n",
        "    det_ymax = results[i][:, 5]\n",
        "\n",
        "    # Get detections with confidence higher than 0.6.\n",
        "    top_indices = [i for i, conf in enumerate(det_conf) if conf >= 0.6]\n",
        "\n",
        "    top_conf = det_conf[top_indices]\n",
        "    top_label_indices = det_label[top_indices].tolist()\n",
        "    top_xmin = det_xmin[top_indices]\n",
        "    top_ymin = det_ymin[top_indices]\n",
        "    top_xmax = det_xmax[top_indices]\n",
        "    top_ymax = det_ymax[top_indices]\n",
        "\n",
        "    colors = plt.cm.hsv(np.linspace(0, 1, 4)).tolist()\n",
        "\n",
        "    plt.imshow(img / 255.)\n",
        "    currentAxis = plt.gca()\n",
        "\n",
        "    for i in range(top_conf.shape[0]):\n",
        "        xmin = int(round(top_xmin[i] * img.shape[1]))\n",
        "        ymin = int(round(top_ymin[i] * img.shape[0]))\n",
        "        xmax = int(round(top_xmax[i] * img.shape[1]))\n",
        "        ymax = int(round(top_ymax[i] * img.shape[0]))\n",
        "        score = top_conf[i]\n",
        "        label = int(top_label_indices[i])\n",
        "#       label_name = voc_classes[label - 1]\n",
        "        display_txt = '{:0.2f}, {}'.format(score, label)\n",
        "        coords = (xmin, ymin), xmax-xmin+1, ymax-ymin+1\n",
        "        color = colors[label]\n",
        "        currentAxis.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=color, linewidth=2))\n",
        "        currentAxis.text(xmin, ymin, display_txt, bbox={'facecolor':color, 'alpha':0.5})\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHcHiBEoLKOW"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}